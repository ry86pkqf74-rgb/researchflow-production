# ==============================================================================
# ResearchFlow Production Environment Variables
# ==============================================================================
#
# INSTRUCTIONS:
# 1. Copy this file to .env in the project root
# 2. Replace placeholder values with your actual credentials
# 3. NEVER commit .env file to version control
# 4. .env is gitignored - keep it that way
#
# WARNING: Exposing API keys can result in unauthorized usage and costs.
# Treat .env file contents as highly sensitive.

# ==============================================================================
# AI PROVIDER API KEYS
# ==============================================================================
#
# CRITICAL: These keys are used for AI-powered features.
# All providers process DE-IDENTIFIED DATA ONLY. NEVER send PHI to external APIs.

# OpenAI API Key (GPT-4, GPT-4o) - General reasoning, summarization
# Get from: https://platform.openai.com/api-keys
OPENAI_API_KEY=

# Anthropic API Key (Claude 3.5 Sonnet) - Structured analysis, long context
# Get from: https://console.anthropic.com/settings/keys
ANTHROPIC_API_KEY=

# Claude API Key (alias for ANTHROPIC_API_KEY in some contexts)
CLAUDE_API_KEY=

# Together AI API Key (Llama, Mixtral, Qwen, DeepSeek) - Cost-effective bulk runs
# Get from: https://api.together.xyz/settings/api-keys
TOGETHER_API_KEY=

# Grok / xAI API Key - Experimental hypothesis ideation
# Get from: https://console.x.ai/ (or xAI developer portal)
XAI_API_KEY=

# InceptionLabs Mercury API Key - Diffusion-based language model
# Get from: https://inceptionlabs.ai/ (Mercury dashboard)
MERCURY_API_KEY=
INCEPTIONLABS_API_KEY=

# Codex API Key (if using separate Codex service)
# Note: OpenAI Codex uses OPENAI_API_KEY
CODEX_API_KEY=

# Sourcegraph API Key - Code search and intelligence
# Get from: https://sourcegraph.com/users/[username]/settings/tokens
SOURCEGRAPH_API_KEY=

# Notion Integration - Database and task tracking
# Get from: https://www.notion.so/my-integrations
NOTION_API_KEY=

# Notion Database IDs for AI Usage Tracking
# These are used by the AI Logger middleware to track API costs
# Get IDs from: Database URL → notion.so/<workspace>/<database-id>
NOTION_API_USAGE_TRACKER_DB=96fe5bba-d3fe-4384-ae9c-0def8a83424d
NOTION_TOOL_USAGE_PLANS_DB=a9c6dde1-7fab-4de9-ba40-9bedeafa62d3

# Disable AI usage logging to Notion (optional)
# DISABLE_AI_LOGGING=true

# Figma API Key - Design-to-code integration
# Get from: https://www.figma.com/developers/api#access-tokens
FIGMA_API_KEY=

# Replit API Token - Rapid prototyping and deployment
# Get from: https://replit.com/account#api
REPLIT_API_TOKEN=

# ==============================================================================
# NCBI / NLM INTEGRATION
# ==============================================================================

# NCBI API Key for MeSH term enrichment and PubMed searches
# Get from: https://www.ncbi.nlm.nih.gov/account/settings/
NCBI_API_KEY=

# Semantic Scholar API Key for enhanced literature search
# Get from: https://www.semanticscholar.org/product/api
SEMANTIC_SCHOLAR_API_KEY=

# Cache TTL for literature search results (seconds)
# Default: 3600 (1 hour)
LITERATURE_CACHE_TTL=3600

# ==============================================================================
# DATABASE CONFIGURATION
# ==============================================================================

# PostgreSQL connection settings (used in docker-compose)
POSTGRES_USER=ros
POSTGRES_PASSWORD=ros
POSTGRES_DB=ros

# ==============================================================================
# AUTHENTICATION & SECURITY
# ==============================================================================

# JWT secret key for authentication
# SECURITY WARNING: Generate a secure random key for production!
# Example: openssl rand -hex 32
JWT_SECRET=development-secret-change-in-production

# JWT token expiration times
JWT_EXPIRES_IN=24h
JWT_REFRESH_EXPIRES_IN=7d

# Allow stateless JWT tokens (useful for development)
# SECURITY WARNING: Set to false in production!
AUTH_ALLOW_STATELESS_JWT=true

# Admin email addresses (comma-separated)
# Users with these emails get ADMIN role automatically on registration
ADMIN_EMAILS=admin@example.com

# ==============================================================================
# GOVERNANCE MODE
# ==============================================================================
#
# Controls system behavior regarding external calls and data processing.
# - LIVE: Full functionality, external AI calls enabled
# - STANDBY: Read-only mode, no external calls

GOVERNANCE_MODE=LIVE

# ==============================================================================
# PHI GOVERNANCE
# ==============================================================================

# Enable PHI (Protected Health Information) scanning
# When true, all content is scanned for PHI patterns
PHI_SCAN_ENABLED=true

# Fail-closed behavior when PHI is detected
# When true and in LIVE mode, operations with PHI are blocked
PHI_FAIL_CLOSED=true

# ==============================================================================
# CHAT AGENTS CONFIGURATION
# ==============================================================================

# LLM model for chat agents
# Options: gpt-4, gpt-4-turbo, claude-3-haiku-20240307, claude-3-sonnet-20240229
CHAT_AGENT_MODEL=gpt-4

# LLM provider for chat agents
# Options: openai, anthropic
CHAT_AGENT_PROVIDER=openai

# Enable chat agents feature in frontend
NEXT_PUBLIC_ENABLE_CHAT_AGENTS=true

# Enable chat agents backend
CHAT_AGENT_ENABLED=true

# ==============================================================================
# DASHBOARD CONFIGURATION (from commit 2b65a72)
# ==============================================================================

# Enable multi-project dashboard
DASHBOARD_ENABLED=true

# Enable calendar integration with milestones/tasks
DASHBOARD_CALENDAR_INTEGRATION=true

# Dashboard auto-refresh interval (milliseconds)
DASHBOARD_REFRESH_INTERVAL=5000

# ==============================================================================
# DATA PARSING (from commit d22b229)
# ==============================================================================

# Enable strict CSV parsing mode for medical datasets
DATA_PARSE_STRICT=true

# ==============================================================================
# ANALYTICS CONFIGURATION (Phase F)
# ==============================================================================

# Salt for hashing IP addresses in analytics (REQUIRED for analytics)
# Generate with: openssl rand -hex 16
ANALYTICS_IP_SALT=

# Sentry DSN for error tracking (optional)
SENTRY_DSN=
VITE_SENTRY_DSN=

# ==============================================================================
# CONFERENCE PREPARATION (Stage 20)
# ==============================================================================

# Path for storing generated artifacts
ARTIFACTS_PATH=/data/artifacts

# Cache TTL for conference discovery (seconds)
# Default: 86400 (24 hours)
CONFERENCE_CACHE_TTL=86400

# Enable web search for conference discovery
# When false, uses demo/mock data only
ENABLE_WEB_SEARCH=false

# ==============================================================================
# WEBHOOKS (External Integrations)
# ==============================================================================

# Stripe webhook signing secret
# Get from: Stripe Dashboard → Developers → Webhooks → Signing secret
STRIPE_WEBHOOK_SECRET=

# Zoom webhook secret token
# Get from: Zoom Marketplace → Your App → Feature → Event Subscriptions
ZOOM_WEBHOOK_SECRET_TOKEN=

# Zoom verification token (for URL validation)
ZOOM_VERIFICATION_TOKEN=

# ==============================================================================
# LARGE-DATA INGESTION CONFIGURATION
# ==============================================================================
#
# These settings control how large files are processed using Dask or chunked reading.

# File size threshold (bytes) above which streaming or Dask is used
# Default: 52428800 (50 MB)
LARGE_FILE_BYTES=52428800

# Number of rows per chunk for pandas.read_csv(..., chunksize=...)
# Default: 500000
CHUNK_SIZE_ROWS=500000

# Enable Dask distributed processing for large files
# Default: false (uses chunked pandas fallback)
DASK_ENABLED=false

# Block size for Dask read_csv partitions
# Accepts byte values (e.g., 64MB, 128MB) or raw integers
# Default: 64MB
DASK_BLOCKSIZE_BYTES=64MB

# Number of Dask worker processes
# Default: 4
DASK_WORKERS=4

# Threads per Dask worker
# Default: 2
DASK_THREADS_PER_WORKER=2

# Memory limit per Dask worker (e.g., 2GB, 4GB)
# Default: 4GB
DASK_MEMORY_LIMIT=4GB

# External Dask scheduler address (leave empty for local cluster)
# Example: tcp://scheduler:8786
DASK_SCHEDULER_ADDR=

# Maximum Parquet partition file size (bytes)
# Default: 104857600 (100 MB)
MAX_PARQUET_FILE_SIZE=104857600

# ==============================================================================
# AI SELF-IMPROVEMENT LOOP CONFIGURATION (Phase 9)
# ==============================================================================
#
# Controls the automatic refinement of AI-generated content.

# Enable the AI self-improvement loop
# When true, failed quality checks trigger automatic prompt refinement
# Default: false (single-pass generation)
AUTO_REFINE_ENABLED=false

# Maximum number of refinement iterations per request
# Default: 3
MAX_REFINE_ATTEMPTS=3

# Number of refinement attempts before recommending tier escalation
# Default: 2
REFINEMENT_ESCALATION_THRESHOLD=2

# Treat quality check warnings as errors (strict mode)
# Default: false
QUALITY_CHECK_STRICT_MODE=false

# Minimum quality score threshold (0.0-1.0) to pass
# Default: 0.7
MIN_QUALITY_SCORE_THRESHOLD=0.7

# Model tier/name for critique phase (optional)
# Example: gpt-4, claude-3-opus
CRITIC_MODEL=

# Model tier/name for refinement phase (optional)
# Example: claude-3-sonnet, gpt-4-turbo
ACTOR_MODEL=

# ==============================================================================
# QUALITY GATE CONFIGURATION (Phase 9)
# ==============================================================================
#
# Controls quality validation for AI-generated content.

# Enable quality gate validation
# Default: true
QUALITY_GATE_ENABLED=true

# Enable tier escalation on quality failure
# Default: true
ESCALATION_ENABLED=true

# Maximum tier escalations per request
# Default: 2
MAX_ESCALATIONS=2

# Task types that require narrative quality checks (comma-separated)
# Default: draft_section,abstract_generate,complex_synthesis
NARRATIVE_TASK_TYPES=draft_section,abstract_generate,complex_synthesis

# Default minimum citations for narrative content
# Default: 3
DEFAULT_MIN_CITATIONS=3

# Default minimum word count for narrative content
# Default: 100
DEFAULT_MIN_WORDS=100

# Default maximum word count for narrative content
# Default: 2000
DEFAULT_MAX_WORDS=2000

# Default model tier for AI tasks
# Options: NANO, MINI, FRONTIER
# Default: MINI
AI_DEFAULT_TIER=MINI

# AI request timeout (milliseconds)
# Default: 120000 (2 minutes)
AI_REQUEST_TIMEOUT_MS=120000

# Maximum retries for transient AI failures
# Default: 3
AI_MAX_RETRIES=3

# Retry backoff delay (milliseconds)
# Default: 1000
AI_RETRY_BACKOFF_MS=1000

# Enable AI debug logging (not for production)
# Default: false
AI_DEBUG_LOGGING=false

# ==============================================================================
# DEMO MODE CONFIGURATION
# ==============================================================================

# Use real LLM generation in demo mode (otherwise returns stub proposals)
# Default: false (use stub proposals for faster demo)
DEMO_USE_REAL_GENERATION=false

# ==============================================================================
# DEVELOPMENT / DEBUG
# ==============================================================================

# Environment: dev, test, or prod
ENVIRONMENT=dev

# Enable verbose logging
DEBUG=false

# ==============================================================================
# END OF CONFIGURATION
# ==============================================================================
