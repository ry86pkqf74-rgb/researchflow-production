"""
AI Model Router Stub

Provides offline AI task routing with mock backends.
No external API calls, network dependencies, or AI service integrations.

This is a stub implementation for establishing routing patterns and governance
compliance before real model integrations are added.
"""

from typing import Dict, Any, Optional
from datetime import datetime
import json


class AIModelRouter:
    """
    Offline AI task router with mock backends.

    Routes tasks to mock functions that simulate AI model responses
    without making external API calls or network requests.

    Supports governance-compliant logging and audit trails.
    """

    def __init__(self, enable_logging: bool = True):
        """
        Initialize the AI router.

        Args:
            enable_logging: Enable audit logging of routing decisions
        """
        self.enable_logging = enable_logging
        self.routing_log = []

    def route(
        self, task: str, context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Route a task to the appropriate mock backend.

        Args:
            task: Task type (e.g., "summarize", "analyze", "code_review")
            context: Optional context data (must not contain PHI/PII)

        Returns:
            Dict containing:
                - response: The mock AI response
                - model: Model identifier used
                - timestamp: ISO 8601 timestamp
                - task: Original task type

        Raises:
            ValueError: If task type is unknown or context contains forbidden data
        """
        if context is None:
            context = {}

        # Validate no PHI/PII in context (basic check)
        self._validate_no_sensitive_data(context)

        # Log routing decision
        routing_entry = {
            "timestamp": datetime.utcnow().isoformat(),
            "task": task,
            "context_keys": list(context.keys()),
        }

        # Route to appropriate mock backend
        if task == "summarize":
            response = self._mock_openai_summary(context)
            routing_entry["model"] = "mock-openai-gpt4"
        elif task == "analyze":
            response = self._mock_anthropic_analysis(context)
            routing_entry["model"] = "mock-anthropic-claude"
        elif task == "code_review":
            response = self._mock_code_review(context)
            routing_entry["model"] = "mock-github-copilot"
        elif task == "literature_screen":
            response = self._mock_literature_screening(context)
            routing_entry["model"] = "mock-openai-gpt4"
            routing_entry["requires_external_tool"] = True
            routing_entry["external_tool"] = "elicit.com"
            routing_entry["human_oversight_required"] = True
        elif task == "debug":
            response = self._mock_debugging_assistant(context)
            routing_entry["model"] = "mock-anthropic-claude"
        else:
            routing_entry["error"] = f"Unknown task type: {task}"
            if self.enable_logging:
                self.routing_log.append(routing_entry)
            raise ValueError(f"Unknown task type: {task}")

        routing_entry["response_length"] = len(response)

        if self.enable_logging:
            self.routing_log.append(routing_entry)

        return {
            "response": response,
            "model": routing_entry["model"],
            "timestamp": routing_entry["timestamp"],
            "task": task,
        }

    def _validate_no_sensitive_data(self, context: Dict[str, Any]) -> None:
        """
        Basic validation to ensure context doesn't contain obvious PHI/PII markers.

        This is a stub implementation. Real validation would be more comprehensive.
        """
        forbidden_keys = [
            "ssn",
            "medical_record_number",
            "patient_name",
            "dob",
            "phi",
            "pii",
        ]
        context_str = json.dumps(context).lower()

        for key in forbidden_keys:
            if key in context_str:
                raise ValueError(
                    f"Context appears to contain sensitive data (found: {key})"
                )

    def _mock_openai_summary(self, context: Dict[str, Any]) -> str:
        """Mock OpenAI GPT-4 summary generation."""
        return (
            "[MOCK RESPONSE - OpenAI GPT-4]\n\n"
            "This is a simulated summary generated by a mock OpenAI backend. "
            "In a production environment, this would call the OpenAI API with "
            "appropriate safety measures and data handling protocols.\n\n"
            f"Task context keys provided: {', '.join(context.keys()) if context else 'None'}\n\n"
            "Note: This is an offline stub. No external API calls were made."
        )

    def _mock_anthropic_analysis(self, context: Dict[str, Any]) -> str:
        """Mock Anthropic Claude analysis generation."""
        return (
            "[MOCK RESPONSE - Anthropic Claude]\n\n"
            "This is a simulated analysis generated by a mock Anthropic backend. "
            "In a production environment, this would call the Anthropic API with "
            "appropriate safety measures and data handling protocols.\n\n"
            f"Task context keys provided: {', '.join(context.keys()) if context else 'None'}\n\n"
            "Analysis points:\n"
            "1. [Mock point 1]\n"
            "2. [Mock point 2]\n"
            "3. [Mock point 3]\n\n"
            "Note: This is an offline stub. No external API calls were made."
        )

    def _mock_code_review(self, context: Dict[str, Any]) -> str:
        """Mock GitHub Copilot code review."""
        return (
            "[MOCK RESPONSE - GitHub Copilot]\n\n"
            "This is a simulated code review generated by a mock GitHub Copilot backend.\n\n"
            "Code quality assessment:\n"
            "- Style: Adheres to PEP 8 conventions\n"
            "- Logic: No obvious bugs detected\n"
            "- Security: No SQL injection or XSS vulnerabilities found\n"
            "- Performance: No obvious bottlenecks\n\n"
            f"Files reviewed: {context.get('file_count', 'N/A')}\n\n"
            "Note: This is an offline stub. No external API calls were made."
        )

    def _mock_literature_screening(self, context: Dict[str, Any]) -> str:
        """Mock literature screening with external tool guidance."""
        query = context.get("query", context.get("focus", "[No query provided]"))

        return (
            "[LITERATURE SCREENING - EXTERNAL TOOL GUIDANCE]\n\n"
            "⚠️ This task requires HUMAN-OVERSEEN external tool usage (Elicit.com)\n"
            "⚠️ NO automatic API calls will be made - you must execute manually\n\n"
            "═══════════════════════════════════════════════════════════════════\n"
            "APPROVED TOOL: Elicit.com (https://elicit.com)\n"
            "GOVERNANCE: See docs/governance/EXTERNAL_LITERATURE_TOOLS.md\n"
            "═══════════════════════════════════════════════════════════════════\n\n"
            f"QUERY FOR ELICIT:\n{query}\n\n"
            "STEP-BY-STEP INSTRUCTIONS:\n\n"
            "1. QUERY ELICIT (Manual)\n"
            "   - Navigate to: https://elicit.com\n"
            f"   - Enter query: {query}\n"
            "   - Review AI-generated paper summaries\n"
            "   - Refine query if needed\n\n"
            "2. EXPORT RESULTS (CSV/Excel)\n"
            "   - Click 'Export' in Elicit interface\n"
            "   - Download to: data/restricted/literature_exports/\n"
            "   - Filename: elicit_export_YYYYMMDD_HHMMSS.csv\n\n"
            "3. REDACT SENSITIVE DATA (Before Committing)\n"
            "   - Remove any API keys, tokens, or credentials\n"
            "   - Strip user-identifying metadata\n"
            "   - Verify no PHI/PII in exported data\n"
            "   - Document redactions in REDACTION_LOG.md\n\n"
            "4. OUT-OF-BAND IMPORT (Gitignored Location)\n"
            "   - Place export in: data/restricted/literature_exports/\n"
            "   - Verify git status shows NO new tracked files\n"
            "   - Process locally (never commit raw exports)\n\n"
            "5. AUDIT TRAIL (Required)\n"
            "   - Log query, export date, redaction steps\n"
            "   - Document in: reports/literature_audit/YYYYMMDD_elicit_query.md\n"
            "   - Include: query text, result count, inclusion criteria\n\n"
            "6. HUMAN REVIEW (Critical)\n"
            "   - Review all paper summaries for relevance\n"
            "   - Apply systematic review inclusion/exclusion criteria\n"
            "   - Document decisions with rationale\n"
            "   - Flag any uncertain papers for discussion\n\n"
            "═══════════════════════════════════════════════════════════════════\n"
            "COMPLIANCE CHECKLIST:\n"
            "═══════════════════════════════════════════════════════════════════\n\n"
            "Before proceeding, verify:\n"
            "☐ No PHI/PII uploaded to Elicit.com\n"
            "☐ Query contains only general medical/scientific terms\n"
            "☐ Export downloaded to gitignored directory\n"
            "☐ Redaction log created and reviewed\n"
            "☐ Audit trail documentation complete\n"
            "☐ Human review of all results performed\n"
            "☐ Git status verified (no literature exports staged)\n\n"
            "═══════════════════════════════════════════════════════════════════\n"
            "RISK MITIGATION:\n"
            "═══════════════════════════════════════════════════════════════════\n\n"
            "✓ No automatic API integration (human-in-the-loop required)\n"
            "✓ Exports stored in gitignored directory (data/restricted/)\n"
            "✓ Redaction process enforced before any Git commits\n"
            "✓ Full audit trail for governance compliance\n"
            "✓ Human oversight required for all literature decisions\n\n"
            "For detailed governance policy, see:\n"
            "docs/governance/EXTERNAL_LITERATURE_TOOLS.md\n\n"
            "Note: This is offline guidance. No external API calls were made.\n"
            "You must execute Elicit.com queries manually per governance policy."
        )

    def _mock_debugging_assistant(self, context: Dict[str, Any]) -> str:
        """Mock debugging assistance."""
        return (
            "[MOCK RESPONSE - Debugging Assistant]\n\n"
            "This is a simulated debugging suggestion.\n\n"
            "Error analysis:\n"
            f"- Error type: {context.get('error_type', 'Unknown')}\n"
            "- Likely cause: [Mock diagnosis]\n"
            "- Suggested fix: [Mock solution]\n"
            "- Related docs: [Mock documentation links]\n\n"
            "Note: This is an offline stub. No external API calls were made."
        )

    def get_routing_log(self) -> list:
        """
        Retrieve the routing audit log.

        Returns:
            List of routing events with timestamps and model selections
        """
        return self.routing_log.copy()

    def get_routing_statistics(self) -> Dict[str, Any]:
        """
        Get aggregated statistics from routing log.

        Returns:
            Dict containing:
                - total_requests: Total number of routing requests
                - requests_by_task: Count of requests per task type
                - requests_by_model: Count of requests per model
                - average_response_length: Average response length in characters
        """
        if not self.routing_log:
            return {
                "total_requests": 0,
                "requests_by_task": {},
                "requests_by_model": {},
                "average_response_length": 0,
            }

        requests_by_task = {}
        requests_by_model = {}
        total_response_length = 0

        for entry in self.routing_log:
            task = entry.get("task")
            model = entry.get("model")
            response_length = entry.get("response_length", 0)

            requests_by_task[task] = requests_by_task.get(task, 0) + 1
            requests_by_model[model] = requests_by_model.get(model, 0) + 1
            total_response_length += response_length

        return {
            "total_requests": len(self.routing_log),
            "requests_by_task": requests_by_task,
            "requests_by_model": requests_by_model,
            "average_response_length": (
                total_response_length // len(self.routing_log)
                if self.routing_log
                else 0
            ),
        }

    def clear_routing_log(self) -> None:
        """
        Clear the routing log.

        Use with caution - this removes audit trail.
        Should only be used in testing or after exporting logs.
        """
        self.routing_log.clear()

    def export_audit_log(self, filepath: str) -> None:
        """
        Export routing log to JSON file for governance compliance.

        Args:
            filepath: Path to output JSON file
        """
        stats = self.get_routing_statistics()

        with open(filepath, "w") as f:
            json.dump(
                {
                    "audit_log": self.routing_log,
                    "statistics": stats,
                    "exported_at": datetime.utcnow().isoformat(),
                    "router_type": "stub",
                    "network_calls_made": False,
                },
                f,
                indent=2,
            )


# Example usage (for documentation)
if __name__ == "__main__":
    router = AIModelRouter(enable_logging=True)

    # Example: Summarization task
    result = router.route("summarize", {"document_length": 1500})
    print(f"Task: {result['task']}")
    print(f"Model: {result['model']}")
    print(f"Response:\n{result['response']}\n")

    # Example: Analysis task
    result = router.route("analyze", {"data_points": 100})
    print(f"Task: {result['task']}")
    print(f"Model: {result['model']}")
    print(f"Response:\n{result['response']}\n")

    # Export audit log
    router.export_audit_log("/tmp/router_audit.json")
    print("Audit log exported to /tmp/router_audit.json")
